{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TFlearn helper to setup neural networks\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "\n",
    "# Tools to scrape the web\n",
    "import re, urllib, requests, os\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "\n",
    "# Data wrangling modules\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to scrape the IGN website for verdicts\n",
    "\n",
    "def get_review_verdict(game_url):\n",
    "    url = 'http://ca.ign.com%s' % (game_url)\n",
    "    response = requests.get(url, headers={'User-agent': 'Mozilla/5.0'})\n",
    "\n",
    "    # parse the search page using SoupStrainer and lxml\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "    # Get the review link\n",
    "    rev_tag = soup.find(name=\"a\", class_=\"reviewLink\", string=re.compile('Review'))\n",
    "    try:\n",
    "        rev_url = rev_tag.attrs['href']\n",
    "\n",
    "        # Extract the verdict of the review\n",
    "        response = requests.get(rev_url, headers={'User-agent': 'Mozilla/5.0'})\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "        verdict_tag = soup.find(name='div', class_='articleSubHeader', string=re.compile('The Verdict'))\n",
    "\n",
    "        verdict = verdict_tag.next_sibling.next_sibling.next_sibling\n",
    "        verdict_str = ''.join([s for s in verdict.strings])\n",
    "    except:\n",
    "        verdict_str = None\n",
    "\n",
    "    return verdict_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read ign data\n",
    "# If the dataset with appended reviews exists read it, if not query \n",
    "\n",
    "if os.path.exists('ign_reviews.csv'):\n",
    "    ignd = pd.read_csv('ign_reviews.csv', index_col=0)\n",
    "else:\n",
    "    ignd = pd.read_csv('ign.csv', index_col=0)\n",
    "    \n",
    "    # Fill in the reviews (this takes a while)\n",
    "    ignd['Reviews'] = ignd['url'].map(lambda x: get_review_verdict(x))\n",
    "    \n",
    "    # Save to file for future use\n",
    "    ignd.to_csv('ign_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop and create bag of words using a Counter\n",
    "# Some reviews were most probably not successfully retrieved so they need to be filtered out\n",
    "\n",
    "bw = Counter()\n",
    "mask = ignd['Reviews'].isnull() == False\n",
    "\n",
    "char_to_sub = '|'.join(['\\(', '\\)', ',', '\\.', '\\?', '!', ])\n",
    "pat = re.compile(char_to_sub) \n",
    "\n",
    "for rev in ignd.loc[mask, 'Reviews']:\n",
    "    rev = rev.lower()\n",
    "    rev = pat.sub('', rev)\n",
    "    words = rev.split()\n",
    "    for w in words:\n",
    "        bw[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a corpus out of the 10000 most frequent words\n",
    "# The corpus is a dictionary with index association\n",
    "\n",
    "corpus = {w[0]:idx+1 for w,idx in zip(bw.most_common(10000), range(1000))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform every review in a sequence of numbers associated with the words\n",
    "\n",
    "# Helper function to convert a text (words) to a sequence of indices\n",
    "def text_to_seq(text):\n",
    "    text.lower()\n",
    "    text = pat.sub('', text)\n",
    "    words = text.split()\n",
    "    idx = [corpus.get(w, -1) for w in words]\n",
    "    return [i for i in idx if i != -1]\n",
    "    \n",
    "wseq = []\n",
    "lengths = []\n",
    "for rev in ignd.loc[mask, 'Reviews']:\n",
    "    idx = text_to_seq(rev)\n",
    "    lengths.append(len(idx))\n",
    "    wseq.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the scores associated with the reviews\n",
    "\n",
    "scores = ignd.score[mask] / 10\n",
    "\n",
    "mask1 = scores <= 1/3\n",
    "mask2 = (scores > 1/3) & (scores <= 2/3)\n",
    "mask3 = scores > 2/3\n",
    "\n",
    "scores[mask1] = 0\n",
    "scores[mask2] = 1\n",
    "scores[mask3] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare data for the neural network\n",
    "\n",
    "# Sequence padding\n",
    "X = pad_sequences(wseq, maxlen=max(lengths), value=0.)\n",
    "# Converting labels to binary vectors\n",
    "Y = to_categorical(scores, nb_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take 90% for training and 10% for testing purposes\n",
    "\n",
    "idx = int(0.9 * X.shape[0])\n",
    "trainX, testX = X[:idx], X[idx:]\n",
    "trainY, testY = Y[:idx], Y[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10277, 277)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Network building function\n",
    "\n",
    "def build_model():\n",
    "    # This resets all parameters and variables, leave this here\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    net = tflearn.input_data([None, X.shape[1]])\n",
    "    net = tflearn.embedding(net, input_dim=10000, output_dim=128)\n",
    "    net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "    net = tflearn.fully_connected(net, 3, activation='softmax')\n",
    "    net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                             loss='categorical_crossentropy')\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3220  | total loss: \u001b[1m\u001b[32m0.33984\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 010 | loss: 0.33984 - acc: 0.8700 | val_loss: 0.62482 - val_acc: 0.7881 -- iter: 10277/10277\n",
      "Training Step: 3220  | total loss: \u001b[1m\u001b[32m0.33984\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 010 | loss: 0.33984 - acc: 0.8700 | val_loss: 0.62482 - val_acc: 0.7881 -- iter: 10277/10277\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Build the network and train\n",
    "\n",
    "net = build_model()\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,\n",
    "          n_epoch=50, batch_size=32)\n",
    "\n",
    "# Save model\n",
    "model.save('sentiment_analysis_rnn.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
